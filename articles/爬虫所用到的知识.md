---
2020.5.11
---
# node爬虫
爬虫要遵守Robots协议，也就是爬虫协议，爬虫程序在爬取网站数据之前，会先看看是否存在robots.txt文件，假如有，会在这个文件允许的
范围内进行爬取。像百度、谷歌都是遵循这一协议的。

根据我的经验，爬虫会有一下几种类型：

## 爬网页
用到的模块request网络请求网页，cheerio类似于jQuery的语法解析网页。


```Javascript
const cheerio = require('cheerio');
const request = require('request')

async function getWordFromIciBa(english) {
    return new Promise((resolve, reject) => {
        request.get(`https://www.iciba.com/${english}`, (err, response, body) => {
            if (!err && response.statusCode === 200) {
                const $ = cheerio.load(body.toString())
                let yinbiao = $('div.base-speak>span:nth-child(2)').text().replace(/\s/g, '').slice(1) //美式音标
                let chinese = $('ul.switch_part li').text().replace(/\s/g, '')  //中文释义
                resolve({
                     us_phonetic: yinbiao,
                     chinese: chinese
                })
            } else {
                reject(err)
            }
        })
    }).catch(err => {
        console.log(err)
    })
}
```

## 模拟登录
用到的模块superagent，需要登录时，在请求头中添加cookie。
例子：爬取qq空间

```Javascript

const superagent = require('superagent')

let url = 'https://user.qzone.qq.com/proxy/domain/taotao.qq.com/cgi-bin/emotion_cgi_msglist_v6'
let pos = 20;
let num = 40;
let fun1 = '_preloadCallback'
let func2 = '_preloadCallback'

async function fix() {
    try {
        let res = await superagent.get(url).set('cookie', '528226249_todaycount=0; 528226249_totalcount=29294; pgv_pvi=5218883584; pgv_si=s8073185280; qz_screen=1440x900; pgv_pvid=7967706718; pgv_info=ssid=s5682998744; QZ_FE_WEBP_SUPPORT=1; cpu_performance_v8=1; RK=oPRBCnNXSR; ptcz=fb06e745fbe330fe99a3447722d205f58030819c78f46a2e842af1b1d423faed; __Q_w_s_hat_seed=1; __Q_w_s__QZN_TodoMsgCnt=1; Loading=Yes; ptui_loginuin=528226249@qq.com; uin=o0528226249; skey=@TaX0jGEVu; p_uin=o0528226249; pt4_token=1o3CiF3KorxeXMslISpIWHSkWu6wYMVGXfrSCs6aJHM_; p_skey=a1MbdGhwOcH7Ax2IE1Ixy-N4kuO6G-khvzM1nx4u*Bo_')
            .query({
                uin: 528226249,
                ftype: 0,
                sort: 0,
                pos: pos,
                num: num,
                replynum: 100,
                g_tk: 846558423,
                callback: '_preloadCallback',
                code_version: 1,
                format: 'jsonp',
                need_private_comment: 1,
                qzonetoken: '656e856329ef0609bdae4fceadd676c8312e4371446fdd95f3f3e8cbe482db5a6c0941fadacf72',
            })
        ;
        let text = res.res.text;
        console.log(text)
        text = text.slice(17)
        text = text.slice(0, -2)
        const obj = JSON.parse(text)
        let msglist = obj.msglist
        // console.log(msglist)

        msglist.forEach(item => {
           // item 为每一条说说
        })

    } catch (e) {
        console.log(e)
    }

}

```

## 导出excel
用到的模块node-xlsx，将数据写入xlsx文件中。可以参考文档： https://www.npmjs.com/package/node-xlsx

例子：

```Javascript
const xlsx = require('node-xlsx');
const data = [[1, 2, 3], [true, false, null, 'sheetjs'], ['foo', 'bar', new Date('2014-02-19T14:30Z'), '0.3'], ['baz', null, 'qux']];
const options = {'!cols': [{wch: 20}, {wch: 20}, {wch: 60}]};
 let buffer = xlsx.build([{
    name: 'sheet1',
    data: data 
}], options)

 fs.writeFile('./my_xlsx.xlsx', buffer, function (err) {
     if (err) throw err
     console.log('Write to my_xlsx.xlsx has finished')
 })


// Parse a file
const workSheetsFromFile = xlsx.parse(`${__dirname}/no_word.xlsx`);
const workSheetsFromBuffer = xlsx.parse(fs.readFileSync(`${__dirname}/no_word.xlsx`));
```
以上是我做Node爬虫遇到的几种类型，以后遇到了新的再补充到下面。

